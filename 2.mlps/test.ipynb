{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, tensor, optim\n",
    "import numpy as np\n",
    "from lib.activations import ReLU, Softmax\n",
    "from lib.dataset import X, y\n",
    "from lib.losses import CrossEntropyLoss\n",
    "from lib.network import Sequential, Linear\n",
    "from lib.network_base import Module\n",
    "from lib.utilities import one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(\n",
    "    [\n",
    "        [6.7776, -3.6296],\n",
    "        [-4.08, 4.42],\n",
    "        [-4.08, 4.42],\n",
    "        [6.1599, -4.1828],\n",
    "    ]\n",
    ")\n",
    "preds = Softmax()(a)\n",
    "Y_onehot = one_hot_encoding(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(\n",
    "        preds,\n",
    "        Y_onehot,\n",
    "        atol=1e-3,\n",
    "        err_msg=f\"The model predicts the wrong classes. Ground-truth: {Y_onehot}, predictions: {preds}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.02132432e-05,  3.02132432e-05],\n",
       "       [ 2.03426978e-04, -2.03426978e-04],\n",
       "       [ 2.03426978e-04, -2.03426978e-04],\n",
       "       [-3.22261531e-05,  3.22261531e-05]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds - Y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.array(\n",
    "    [\n",
    "        [3.21,-2.34, 3.16],\n",
    "        [3.21,-2.34, 3.16],\n",
    "    ]\n",
    ")\n",
    "b1 = np.array([-3.21, 2.34, -3.16])\n",
    "\n",
    "w2 = np.array(\n",
    "    [\n",
    "        [0.03, -5.84],\n",
    "        [4.64, -3.44],\n",
    "        [3.21, 3.21],\n",
    "    ]\n",
    ")\n",
    "b2 = np.array([-4.08,4.42])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(\n",
    "    [\n",
    "        [6.7776, -3.6296],\n",
    "        [-4.08, 4.42],\n",
    "        [-4.08, 4.42],\n",
    "        [6.1599, -4.1828],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2 = np.array(\n",
    "#     [\n",
    "#         [(10.2399/3.21)-3.16, -(8.6028/3.21)-3.16],\n",
    "#         [10.8576/2.34, -8.0496/2.34],\n",
    "#         [3.21, 3.21],\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.7776, -3.6296],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [ 6.1599, -4.1828]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum((X @ w1) + b1,0) @ w2 +b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.7776, -3.6296],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [ 6.1599, -4.1828]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.6976,  0.7904],\n",
       "       [-8.16  ,  8.84  ],\n",
       "       [-8.16  ,  8.84  ],\n",
       "       [ 2.0799,  0.2372]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\n",
    "    [1,0,0],\n",
    "    [0,0,1],\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "]) @ np.array([\n",
    "    [2.6976,0.7904],\n",
    "    [2.0799,  0.2372],\n",
    "    [-8.16, 8.84],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[ 2.6976,  0.7904],\n",
    "       [-8.16  ,  8.84  ],\n",
    "       [-8.16  ,  8.84  ],\n",
    "       [ 2.0799,  0.2372]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.layers = [\n",
    "            Linear(2, 2),\n",
    "            ReLU(),\n",
    "            Linear(2, 1),\n",
    "        ]\n",
    "        # weight 1\n",
    "        self.layers[0].W.data = np.array(\n",
    "            [\n",
    "                [3.21, -2.34],\n",
    "                [3.21, -2.34],\n",
    "            ]\n",
    "        )\n",
    "        # bias 1\n",
    "        self.layers[0].b.data = np.array([-3.21, 2.34])\n",
    "        # weight 2\n",
    "        self.layers[2].W.data = np.array(\n",
    "            [\n",
    "                [3.19, -2.68],\n",
    "                [4.64, -3.44],\n",
    "            ]\n",
    "        )\n",
    "        # bias 2\n",
    "        self.layers[2].b.data = np.array([-4.08, 4.42])\n",
    "    def forward(self, X):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            X = layer(X)\n",
    "            print(i, X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(2, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2, 1),\n",
    "            ]\n",
    "        )\n",
    "        self.layers[0].weight = nn.Parameter(\n",
    "            tensor(\n",
    "                [\n",
    "                    [3.21, -2.34],\n",
    "                    [3.21, -2.34],\n",
    "                ]\n",
    "            ).T,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.layers[0].bias = nn.Parameter(tensor([-3.21, 2.34]), requires_grad=True)\n",
    "\n",
    "        self.layers[2].weight = nn.Parameter(\n",
    "            tensor(\n",
    "                [\n",
    "                    [3.19, -2.68],\n",
    "                    [4.64, -3.44],\n",
    "                ]\n",
    "            ).T,\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.layers[2].bias = nn.Parameter(tensor([-4.08, 4.42]), requires_grad=True)\n",
    "    def forward(self, X):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            X = layer(X)\n",
    "            print(i, X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net2 = Net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight Parameter containing:\n",
      "tensor([[ 3.2100,  3.2100],\n",
      "        [-2.3400, -2.3400]], requires_grad=True)\n",
      "layers.0.bias Parameter containing:\n",
      "tensor([-3.2100,  2.3400], requires_grad=True)\n",
      "layers.2.weight Parameter containing:\n",
      "tensor([[ 3.1900,  4.6400],\n",
      "        [-2.6800, -3.4400]], requires_grad=True)\n",
      "layers.2.bias Parameter containing:\n",
      "tensor([-4.0800,  4.4200], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = tensor(X,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[-3.2100,  2.3400],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 3.2100, -2.3400]], grad_fn=<AddmmBackward0>)\n",
      "1 tensor([[0.0000, 2.3400],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [3.2100, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "2 tensor([[ 6.7776, -3.6296],\n",
      "        [-4.0800,  4.4200],\n",
      "        [-4.0800,  4.4200],\n",
      "        [ 6.1599, -4.1828]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.7776, -3.6296],\n",
       "        [-4.0800,  4.4200],\n",
       "        [-4.0800,  4.4200],\n",
       "        [ 6.1599, -4.1828]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-3.21  2.34]\n",
      " [ 0.    0.  ]\n",
      " [ 0.    0.  ]\n",
      " [ 3.21 -2.34]]\n",
      "1 [[0.   2.34]\n",
      " [0.   0.  ]\n",
      " [0.   0.  ]\n",
      " [3.21 0.  ]]\n",
      "2 [[ 6.7776 -3.6296]\n",
      " [-4.08    4.42  ]\n",
      " [-4.08    4.42  ]\n",
      " [ 6.1599 -4.1828]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.7776, -3.6296],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [-4.08  ,  4.42  ],\n",
       "       [ 6.1599, -4.1828]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.ModuleList(\n",
    "    [\n",
    "        nn.Linear(3, 2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(2, 1),\n",
    "        nn.Sigmoid(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(3, 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2, 1),\n",
    "                nn.Sigmoid(),\n",
    "            ]\n",
    "        )\n",
    "        self.layers[0].weight = nn.Parameter(\n",
    "            tensor(\n",
    "                [\n",
    "                    [-2.0, 2.0, -3.0],\n",
    "                    [1.0, 0.0, 1.0],\n",
    "                ]\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.layers[0].bias = nn.Parameter(tensor([3.0, 0.0]), requires_grad=True)\n",
    "\n",
    "        self.layers[2].weight = nn.Parameter(tensor([[-1.0, 1.0]]), requires_grad=True)\n",
    "        self.layers[2].bias = nn.Parameter(tensor([-3.0]), requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            X = layer(X)\n",
    "            print(i, X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tensor(\n",
    "    [\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [3.0, 4.0, 5.0],\n",
    "    ]\n",
    ")\n",
    "y = tensor([[0.0, 1.0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "lossfun = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = net(X)\n",
    "loss = lossfun(y_hat, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
